# Useful commands - S.Hu 
# Last updated Aug 2020

# LOGIN
ssh sarahhu@poseidon.whoi.edu
### IF using screen - note that there is a l1 and l2 log-in node


# PATH & DIRECTORY INFORMATION
cd /vortexfs1/omics/huber/

# Print working directory
pwd

# Location of scratch
cd $SCRATCH #migrate to scratch
cd /vortexfs1/scratch/sarahhu/ # scratch - keep it tidy

# To make a directory:
mkdir nameofdirectory

# View contents of directory:
ls
ls -lht


####################
## Moving stuff ####
## Transfer files ##
####################
# Take files from location AAA to BBB:
scp AAA/files BBB/

# If AAA = clustering_tutorial.Rmd and I want to move it to my desktop:
# First migrate on your local computer to your desktop (in terminal)
cd usr/sarahscomputer/Desktop/

# use the 'pwd' command to see where the original file is located. You want the whole path
# On your local computer, perform scp:
scp sarahhu@poseidon.whoi.edu:/vortexfs1/omics/huber/shu/18S_pipeline_V4/clustering_tutorial.Rmd . 
## you will be prompted for password

# Between server space:
## From the CDEBI server, I want to bring "list_blast.py" to my space on poseidon:

scp huberlab@kuat.usc.edu:/media/executor/huberlab/shu/python/list_blast.py sarahhu@poseidon.whoi.edu:/vortexfs1/omics/huber/shu

# If I am currently on poseidon, I can ask scp to place the file in my current position with a period (.)
scp huberlab@kuat.usc.edu:/media/executor/huberlab/shu/python/list_blast.py .

###################
# Poseidon status #
###################

sinfo # cluster information
squeue #show all jobs
squeue -u sarahhu # use to look at what you are running (only your jobs)

# If you need to cancel a job:
scancel jobid
# You can find the jobid number by using squeue

shelp

# Submitting jobs:
sbatch submit.script # submits job, more below on this script
scancel <job id> # cancel <job id>
hq     # shows home quota and usage
srun -p interactive -N <nodes> -n <cores> --mem=4gb --pty bash # interactive request


# WHAT PROGRAMS ARE AVAILABLE
module list # What do I have loaded?
module av #W hat is available to me?
## LOAD BIOLOGY
module load bio
module av
module list

# ANACONDA
module load anaconda/5.1
module list
conda info --envs
# https://alexanderlabwhoi.github.io/post/anaconda-r-sarah/


#######################################
## Best practices for poseidon usage ##
#######################################

## Interactive
## Do not do ANYTHING without first entering interactice or scavenger
# the command is 'srun'
srun -p P -N X -n Y --gres=G:n --mem=M --pty bash
# P=partition
# X=nodes
# Y=cores
# G=general resource
# n=number
# --mem=memory in MB (default) or gb (specify) --pty bash
# e.g. srun -p interactive -N 1 -n 2 --mem=4gb --pty bash
# E.g. srun -p gpu -N 1 -n 8 --mem=96gb --gres=gpu:4 --pty bash

# Use to debug, check code, test, test memory
srun -p interactive -N 1 –n 1  --mem=1024 --pty bash
srun -p interactive --mem=500 --ntasks=1 --time=00:20:00 --pty bash

# -N # 1 node
# -n 1 # 1 CPU
# --mem=1024 # 1GB=1024MB

## SCAVENGER
srun -p scavenger --time=03:00:00 --ntasks-per-node 2 --pty bash

## history pro tip!
history | grep "keyword2find"


# IDEAL WORKFLOW:
1. Set up my working environment: files in correct directories, conda environment specs, ensure all programs are working properly and are loaded in environment.
2. Enter interactive or scavenger (set a timer...) to start testing and compiling base code
2a. begin SLURM script construction
3. Test memory of script to run (i.e. SLURM or snakemake dry run)
3a. prep dataset by making a subset
3b. conduct dry run
4. submit script! either with sbatch or gen-profile with snakemake


##################
##EXAMPLE SLURM!##
##################

#!/bin/bash
#SBATCH --partition=compute           # Queue selection
#SBATCH --job-name=fastqc_demo        # Job name
#SBATCH --mail-type=ALL               # Mail events (BEGIN, END, FAIL, ALL)
#SBATCH --mail-user=ahuang@whoi.edu    # Where to send mail
#SBATCH --ntasks=1                    # Run a single task
#SBATCH --cpus-per-task=8             # Number of CPU cores per task
#SBATCH --mem=20000                     # Job memory request
#SBATCH --time=01:00:00               # Time limit hrs:min:sec
#SBATCH --output=fastqc_demo_%j.log   # Standard output/error
#export OMP_NUM_THREADS=8
module load bio fastqc
testcase=/vortexfs1/scratch/ahuang/demo_2
cd $testcase
for i in $testcase/input/*
do fastqc -o $testcase/output -f fastq $i -t 8 --nogroup
done

#EOF



## Setting up to run mothur

1. Create your own conda environment called 'mothur'
```
conda create --name mothur
```

2. Activate this environment
```
conda activate mothur
```
Your command line should now look like this each line preceeding with '(mothur)', instead of base:
```
(mothur) [sarahhu@poseidon-l1 huber-lab]$ 
```

3. Install the latest version of mothur
```
conda install -c bioconda mothur
```

4. Test to make sure install was successful
```
# Start mothur
mothur

# Command line should now look like this:

mothur > 

# Quit
quit()

```

To exit this mothur environment, ```conda deactivate mothur```

*Now*, whenever you want to run mothur, type ```conda activate mothur```, and this mothur enabled environment will launch.

5. To run mothur interactively, start up an interactive or scavenger session. See above instructions.   
